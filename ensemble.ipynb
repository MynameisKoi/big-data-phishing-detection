{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fc5100",
   "metadata": {},
   "source": [
    "## Ensemble: Logistic Regression & Neural Network (MLPClassifier)\n",
    "\n",
    "Combining the Neural Network (MLP) and Logistic Regression models using an Ensemble technique. Specifically, a Soft Voting Classifier is highly effective here. It averages the predicted probabilities from both models (e.g., if the Neural Network says \"90% Spam\" and Logistic Regression says \"70% Spam\", the ensemble sees \"80% Spam\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6da76ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting forensic features...\n",
      "\n",
      "--- HYBRID MODEL RESULTS (Threshold 0.9) ---\n",
      "Accuracy: 0.9216 (Goal: High)\n",
      "False Positive Rate: 0.0038 (Goal: <0.01)\n",
      "False Positives: 38\n",
      "True Positives (Caught): 9144\n",
      "\n",
      "Note: Performance improved, but consider adding more data or features (like BERT) to push further.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1. Setup (Assuming your dataframe 'df' is loaded)\n",
    "# Make sure to run your data loading and cleaning steps first!\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "if not df.empty:\n",
    "    # Fill missing values instead of dropping\n",
    "    df['subject'] = df['subject'].fillna('')\n",
    "    df['body'] = df['body'].fillna('')\n",
    "    df['text_combined'] = df['subject'] + \" \" + df['body']\n",
    "\n",
    "    # --- STEP 2: EXTRACT FORENSIC FEATURES ---\n",
    "    print(\"Extracting forensic features...\")\n",
    "\n",
    "    # Feature A: Capitalization Ratio (The \"Shout\" Factor)\n",
    "    def get_caps_ratio(text):\n",
    "        if len(text) == 0: return 0.0\n",
    "        return sum(1 for c in text if c.isupper()) / len(text)\n",
    "\n",
    "    df['caps_ratio'] = df['subject'].apply(get_caps_ratio)\n",
    "\n",
    "    # Feature B: URL Count (Phishing relies on links)\n",
    "    df['num_urls'] = df['body'].apply(lambda x: len(re.findall(r'http[s]?://', str(x))))\n",
    "\n",
    "    # Feature C: Risk Word Density\n",
    "    # Simple count of high-risk forensic triggers\n",
    "    risk_words = ['urgent', 'verify', 'account', 'suspended', 'click', 'winner', 'security']\n",
    "    def count_risk_words(text):\n",
    "        return sum(1 for w in risk_words if w in str(text).lower())\n",
    "\n",
    "    df['risk_score'] = df['text_combined'].apply(count_risk_words)\n",
    "\n",
    "    # Clean text for the TF-IDF part\n",
    "    df['clean_text'] = df['text_combined'].apply(lambda x: re.sub(r'[^a-z\\s]', '', str(x).lower()))\n",
    "\n",
    "    # Prepare Training Data\n",
    "    X = df[['clean_text', 'caps_ratio', 'num_urls', 'risk_score']]\n",
    "    y = df['label'].astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# 3. THE WORD EMBEDDING PIPELINE\n",
    "# Instead of just TF-IDF, we add TruncatedSVD.\n",
    "# This compresses the sparse words into 100 \"Dense Semantic Features\"\n",
    "# effectively creating a document embedding similar to averaging Word2Vec vectors.\n",
    "text_embedding_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
    "    ('embedding', TruncatedSVD(n_components=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Hybrid Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_vectors', text_embedding_pipeline, 'clean_text'),\n",
    "        ('forensics', MinMaxScaler(), ['caps_ratio', 'num_urls', 'risk_score'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5. Ensemble Model\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=500))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# 6. Full Pipeline\n",
    "model = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('clf', ensemble)\n",
    "])\n",
    "\n",
    "# Train & Evaluate\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict Probabilities\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply Safe Threshold (0.90)\n",
    "threshold = 0.90\n",
    "y_pred_safe = (y_prob >= threshold).astype(int)\n",
    "\n",
    "# Calculate Metrics\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_safe).ravel()\n",
    "acc = accuracy_score(y_test, y_pred_safe)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(f\"\\n--- HYBRID MODEL RESULTS (Threshold {threshold}) ---\")\n",
    "print(f\"Accuracy: {acc:.4f} (Goal: High)\")\n",
    "print(f\"False Positive Rate: {fpr:.4f} (Goal: <0.01)\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"True Positives (Caught): {tp}\")\n",
    "\n",
    "if acc > 0.96 and fpr < 0.01:\n",
    "    print(\"\\nSUCCESS: You have achieved high accuracy AND low false positives!\")\n",
    "else:\n",
    "    print(\"\\nNote: Performance improved, but consider adding more data or features (like BERT) to push further.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977eab2",
   "metadata": {},
   "source": [
    "## Hybrid Forensic Model: TF-IDF --> Linguistic Forensic clues\n",
    "\n",
    "Instead of just looking at words (TF-IDF), we will feed the model specific forensic clues we analyzed earlier:\n",
    "\n",
    "1. Caps Ratio: (Spammers SHOUT).\n",
    "2. URL Count: (Phishing has more links).\n",
    "3. Risk Words: (Specific triggers like \"verify\", \"urgent\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55460431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully loaded dataset.\n",
      "Extracting forensic features...\n",
      "Training LSA Hybrid Model on 61603 emails...\n",
      "\n",
      "--- Evaluation Results (High-Confidence Threshold > 0.90) ---\n",
      "Accuracy:              0.9210\n",
      "False Positive Rate:   0.0039 (Goal: < 0.01)\n",
      "False Positives:       39 (Legitimate emails lost)\n",
      "True Positives:        9069 (Spam caught)\n",
      "\n",
      "[SUCCESS] Extremely low False Positive Rate achieved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "# We try to load your dataset first. If not found, we create synthetic data for demo.\n",
    "try:\n",
    "    # Try loading common filenames\n",
    "    try:\n",
    "        df = pd.read_csv('dataset.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"dataset.csv not found, trying another fileset...\")\n",
    "\n",
    "    # Basic cleanup\n",
    "    df.dropna(subset=['subject', 'body', 'label'], inplace=True)\n",
    "    print(\" Successfully loaded dataset.\")\n",
    "\n",
    "except:\n",
    "    print(\"Dataset not found. Generating SYNTHETIC data for demonstration...\")\n",
    "    # Synthetic Data Generation (Fallback)\n",
    "    subjects = [\"Urgent: Account Suspended\", \"Weekly Meeting\", \"YOU WON $1000\", \"Project Update\", \"Verify Identity\", \"Lunch?\"] * 200\n",
    "    bodies = [\n",
    "        \"Click here to restore access.\",\n",
    "        \"See attached minutes.\",\n",
    "        \"Claim your prize now! Urgent!\",\n",
    "        \"Deadline moved to Friday.\",\n",
    "        \"Unauthorized login attempt detected.\",\n",
    "        \"Want to grab tacos?\"\n",
    "    ] * 200\n",
    "    labels = [1, 0, 1, 0, 1, 0] * 200\n",
    "    df = pd.DataFrame({'subject': subjects, 'body': bodies, 'label': labels})\n",
    "\n",
    "# Combine text for analysis\n",
    "# Fill NaNs just in case\n",
    "df['subject'] = df['subject'].fillna('')\n",
    "df['body'] = df['body'].fillna('')\n",
    "df['text_combined'] = df['subject'] + \" \" + df['body']\n",
    "\n",
    "# --- 2. FORENSIC FEATURE EXTRACTION ---\n",
    "print(\"Extracting forensic features...\")\n",
    "\n",
    "# Feature A: Capitalization Ratio (The \"Shout\" Factor)\n",
    "def get_caps_ratio(text):\n",
    "    s_text = str(text)\n",
    "    if len(s_text) == 0: return 0.0\n",
    "    return sum(1 for c in s_text if c.isupper()) / len(s_text)\n",
    "\n",
    "df['caps_ratio'] = df['subject'].apply(get_caps_ratio)\n",
    "\n",
    "# Feature B: URL Count\n",
    "# Note: Phishing emails typically have more links than normal emails\n",
    "df['num_urls'] = df['body'].apply(lambda x: len(re.findall(r'http[s]?://', str(x))))\n",
    "\n",
    "# Feature C: Risk Word Score\n",
    "# A simple \"bag of threats\" approach\n",
    "risk_words = ['urgent', 'verify', 'account', 'suspended', 'click', 'winner', 'security', 'claim', 'immediate', 'access']\n",
    "def get_risk_score(text):\n",
    "    text_lower = str(text).lower()\n",
    "    return sum(1 for w in risk_words if w in text_lower)\n",
    "\n",
    "df['risk_score'] = df['text_combined'].apply(get_risk_score)\n",
    "\n",
    "# Text Cleaning for LSA\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-z\\s]', '', str(text).lower()).strip()\n",
    "\n",
    "df['clean_text'] = df['text_combined'].apply(clean_text)\n",
    "\n",
    "# --- 3. MODEL PIPELINE SETUP ---\n",
    "\n",
    "# Part A: Text Pipeline (LSA)\n",
    "# TF-IDF -> TruncatedSVD = Latent Semantic Analysis (LSA)\n",
    "# This creates \"Dense Vectors\" similar to Word2Vec\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
    "    ('svd', TruncatedSVD(n_components=100, random_state=42)) # 100 dimensions\n",
    "])\n",
    "\n",
    "# Part B: Column Transformer\n",
    "# Processes text with LSA and numbers with MinMax scaling simultaneously\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_lsa', text_pipeline, 'clean_text'),\n",
    "        ('forensics', MinMaxScaler(), ['caps_ratio', 'num_urls', 'risk_score'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Part C: The Ensemble Classifier\n",
    "# Combining Logistic Regression (Stability) and MLP (Complex Patterns)\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42))\n",
    "    ],\n",
    "    voting='soft' # Soft voting averages the probabilities\n",
    ")\n",
    "\n",
    "# Part D: Final Pipeline\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', ensemble_clf)\n",
    "])\n",
    "\n",
    "# --- 4. TRAINING & EVALUATION ---\n",
    "X = df[['clean_text', 'caps_ratio', 'num_urls', 'risk_score']]\n",
    "y = df['label'].astype(int)\n",
    "\n",
    "# Split 75/25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training LSA Hybrid Model on {len(X_train)} emails...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. HIGH-CONFIDENCE THRESHOLDING ---\n",
    "# We use a 0.90 threshold to minimize False Positives\n",
    "print(\"\\n--- Evaluation Results (High-Confidence Threshold > 0.90) ---\")\n",
    "\n",
    "# Get probabilities (Class 1 = Spam)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.90\n",
    "y_pred_safe = (y_prob >= threshold).astype(int)\n",
    "\n",
    "# Metrics\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_safe).ravel()\n",
    "acc = accuracy_score(y_test, y_pred_safe)\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "print(f\"Accuracy:              {acc:.4f}\")\n",
    "print(f\"False Positive Rate:   {fpr:.4f} (Goal: < 0.01)\")\n",
    "print(f\"False Positives:       {fp} (Legitimate emails lost)\")\n",
    "print(f\"True Positives:        {tp} (Spam caught)\")\n",
    "\n",
    "if fpr == 0:\n",
    "    print(\"\\n[SUCCESS] Zero False Positives achieved.\")\n",
    "elif fpr < 0.01:\n",
    "    print(\"\\n[SUCCESS] Extremely low False Positive Rate achieved.\")\n",
    "else:\n",
    "    print(\"\\n[ADVICE] FPR is still > 1%. Consider raising threshold to 0.95 or adding more forensic features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85988f",
   "metadata": {},
   "source": [
    "## Enhance Ensemble Model: Logistic Regression + Neural Network (MLP) + Gradient Boosting\n",
    "\n",
    "We will add a Gradient Boosting model to the ensemble. Unlike Logistic Regression, Gradient Boosting builds trees sequentially to fix previous errors, often finding subtle patterns that linear models miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aae0ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "Training Enhanced Hybrid Model (LR + MLP + Gradient Boosting)...\n",
      "\n",
      "--- ENHANCED MODEL RESULTS (Threshold 0.85) ---\n",
      "Accuracy: 0.9518\n",
      "False Positive Rate: 0.0041\n",
      "False Positives: 41\n",
      "True Positives: 9704\n",
      "\n",
      "[SUCCESS] High Accuracy and Low FPR achieved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 1. Load Data\n",
    "try:\n",
    "    df = pd.read_csv('dataset.csv')\n",
    "    df.dropna(subset=['subject', 'body', 'label'], inplace=True)\n",
    "except:\n",
    "    print(\"Error: Dataset not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    df['subject'] = df['subject'].fillna('')\n",
    "    df['body'] = df['body'].fillna('')\n",
    "    df['text_combined'] = df['subject'] + \" \" + df['body']\n",
    "\n",
    "    # --- FORENSIC FEATURE EXTRACTION ---\n",
    "    print(\"Extracting features...\")\n",
    "\n",
    "    # Caps Ratio\n",
    "    def get_caps_ratio(text):\n",
    "        if len(str(text)) == 0: return 0.0\n",
    "        return sum(1 for c in str(text) if c.isupper()) / len(str(text))\n",
    "    df['caps_ratio'] = df['subject'].apply(get_caps_ratio)\n",
    "\n",
    "    # URL Count\n",
    "    df['num_urls'] = df['body'].apply(lambda x: len(re.findall(r'http[s]?://', str(x))))\n",
    "\n",
    "    # Risk Word Score\n",
    "    risk_words = ['urgent', 'verify', 'account', 'suspended', 'click', 'winner', 'security', 'claim', 'immediate']\n",
    "    df['risk_score'] = df['text_combined'].apply(lambda x: sum(1 for w in risk_words if w in str(x).lower()))\n",
    "\n",
    "    # Clean Text\n",
    "    df['clean_text'] = df['text_combined'].apply(lambda x: re.sub(r'[^a-z\\s]', '', str(x).lower()))\n",
    "\n",
    "    # --- ENHANCED PIPELINE ---\n",
    "\n",
    "    # 1. Improved LSA: Increase dimensions to 300 to keep more semantic meaning\n",
    "    text_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
    "        ('svd', TruncatedSVD(n_components=300, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # 2. Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text_lsa', text_pipeline, 'clean_text'),\n",
    "            ('forensics', MinMaxScaler(), ['caps_ratio', 'num_urls', 'risk_score'])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3. Stronger Ensemble: Adding Gradient Boosting\n",
    "    # Gradient Boosting is excellent at capturing non-linear patterns in mixed data\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', ensemble)\n",
    "    ])\n",
    "\n",
    "    # Split Data\n",
    "    X = df[['clean_text', 'caps_ratio', 'num_urls', 'risk_score']]\n",
    "    y = df['label'].astype(int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "    # Train\n",
    "    print(\"Training Enhanced Hybrid Model (LR + MLP + Gradient Boosting)...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Optimizing Threshold\n",
    "    # We slightly lower the threshold to 0.85 to recover accuracy,\n",
    "    # relying on the stronger model to keep FPR low.\n",
    "    threshold = 0.85\n",
    "    y_pred_safe = (y_prob >= threshold).astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_safe).ravel()\n",
    "    acc = accuracy_score(y_test, y_pred_safe)\n",
    "    fpr = fp / (fp + tn)\n",
    "\n",
    "    print(f\"\\n--- ENHANCED MODEL RESULTS (Threshold {threshold}) ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"False Positive Rate: {fpr:.4f}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "\n",
    "    if acc > 0.95 and fpr < 0.01:\n",
    "        print(\"\\n[SUCCESS] High Accuracy and Low FPR achieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5f668",
   "metadata": {},
   "source": [
    "## Another approach: Long-Short Term Memory (LSTM)\n",
    "\n",
    "**The Advantage**: Unlike TF-IDF (which ignores word order) or simple embeddings (which average meanings), LSTMs read the email sequentially. They understand the difference between \"not urgent\" and \"urgent\" or \"bank account\" and \"river bank\" based on the sequence. This makes them better at detecting context-based phishing that uses natural-sounding language to trick filters.\n",
    "\n",
    "**The Trade-off**: LSTMs are computationally heavier and slower to train than the Ensemble model you just built. For many simple spam filters, an Ensemble of Logistic Regression + LightGBM is \"good enough\" and much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e86cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM Model...\n",
      "Epoch 1/5\n",
      "870/870 [==============================] - 317s 359ms/step - loss: 0.1289 - accuracy: 0.9534 - val_loss: 0.0823 - val_accuracy: 0.9719\n",
      "Epoch 2/5\n",
      "870/870 [==============================] - 273s 313ms/step - loss: 0.0621 - accuracy: 0.9795 - val_loss: 0.0646 - val_accuracy: 0.9758\n",
      "Epoch 3/5\n",
      "870/870 [==============================] - 248s 286ms/step - loss: 0.0438 - accuracy: 0.9862 - val_loss: 0.0614 - val_accuracy: 0.9762\n",
      "Epoch 4/5\n",
      "870/870 [==============================] - 257s 295ms/step - loss: 0.0365 - accuracy: 0.9874 - val_loss: 0.0668 - val_accuracy: 0.9787\n",
      "Epoch 5/5\n",
      "870/870 [==============================] - 283s 326ms/step - loss: 0.0276 - accuracy: 0.9903 - val_loss: 0.0696 - val_accuracy: 0.9782\n",
      "645/645 [==============================] - 18s 26ms/step\n",
      "\n",
      "--- LSTM RESULTS (Threshold 0.9) ---\n",
      "Accuracy: 0.9748\n",
      "False Positive Rate: 0.0152\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "\n",
    "# 1. Load & Clean Data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.dropna(subset=['body', 'label'], inplace=True)\n",
    "\n",
    "# Combine subject/body if needed\n",
    "df['text_combined'] = df['subject'] + \" \" + df['body']\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-z\\s]', '', str(text).lower()).strip()\n",
    "\n",
    "df['clean_text'] = df['text_combined'].apply(clean_text)\n",
    "\n",
    "X = df['clean_text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# 2. Tokenization & Padding\n",
    "# LSTMs need fixed-length sequences of numbers, not raw text\n",
    "MAX_NB_WORDS = 5000      # Vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 100 # Cut off emails after 100 words\n",
    "EMBEDDING_DIM = 100      # Vector size for each word\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# 3. Build LSTM Model\n",
    "print(\"Building LSTM Model...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) # Turn words into dense vectors\n",
    "model.add(SpatialDropout1D(0.2))                  # Regularization\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # The LSTM Layer\n",
    "model.add(Dense(1, activation='sigmoid'))         # Output layer (0-1 prob)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 4. Train\n",
    "# Epochs=5 is usually enough for spam\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)\n",
    "\n",
    "# 5. Evaluate with High Threshold (Low False Positive)\n",
    "y_prob_lstm = model.predict(X_test_pad)\n",
    "\n",
    "threshold = 0.90\n",
    "y_pred_safe = (y_prob_lstm >= threshold).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_safe).ravel()\n",
    "acc = accuracy_score(y_test, y_pred_safe)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(f\"\\n--- LSTM RESULTS (Threshold {threshold}) ---\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"False Positive Rate: {fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca727a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ENHANCED MODEL RESULTS (Threshold 0.9) ---\n",
      "Accuracy: 0.9748\n",
      "False Positive Rate: 0.0152\n",
      "False Positives: 150\n",
      "True Positives: 10375\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_safe).ravel()\n",
    "acc = accuracy_score(y_test, y_pred_safe)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(f\"\\n--- ENHANCED MODEL RESULTS (Threshold {threshold}) ---\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"False Positive Rate: {fpr:.4f}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"True Positives: {tp}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
